\chapter{Experimental Evaluation}
All experiments were run on an Ubuntu virtual machine with 1 processor core and 1024 MB of memory.

\section{Measures}
\subsection{Supervised learning}
The supervised algorithms were evaluated in two ways: cross-validation and test data.

\textbf{Cross-validation}: I used $k$-fold cross validation to evaluate each model.  The training data is split into $k$ equal size partitions, with $k-1$ subsets used to train the model, and the remaining subset used for validation.  This is repeated $k$ times and produces $k$ different accuracy scores, which may be averaged.

I chose $k=3$ due to my small dataset.

\textbf{See what happens when you use StratifiedKFold}

\textbf{Test data}:  To maintain the integrity of my validation analysis, I ensured that my testing and training data sets were distinct.

To test Model 1, I aggregated open source implementations of various crypto algorithms, as well as programs that do not contain crypto (\textbf{this is kinda a todo)}.

To test Models 2 and 3, I compiled my training set with gcc, then compiled the same set of files with clang.  This emulates the idea of a basic block and isolates the crypto.  When these models were tested against larger programs, they performed poorly without this isolation.

After fitting the model, I evaluated three measures of correctness: precision, recall, and f1-score (averaged over all classes).  Consider the true positives ($tp$), true negatives ($tn$), false positives ($fp$), and false negatives ($fn$).
\begin{description}
\item[Precision]: $\frac{tp}{(tp+fp)}$
\item[Recall]: $\frac{tp}{(tp+fn)}$
\item[F1-score]: The harmonic mean of the precision and real $$f1=2\times(\text{precision}\times\text{recall})/ (\text{precision}+\text{recall})$$
\end{description}

\subsection{Unsupervised learning}
To evaluate the models generated by kmeans clustering, I considered three metrics: homogeneity, completeness, and v-score.

\begin{description}
	\item[Homogeneity]: A cluster is homogeneous (value=1.0) if all clusters contain data points which belong to a single class (requires true labeled data)
	\item[Completeness]: A cluster is complete (value = 1.0) if all data points of a class are elements of the same cluster
	\item[V-score]: The harmonic mean between homogeneity and completeness $$v=2\times(\text{homogeneity}\times\text{completeness})/ (\text{homogeneity}+\text{completeness})$$
\end{description}

\section{Experimental Results}
\subsection{SVM kernel Results}
To determine the optimal kernel function, I used 3-fold cross-validation on a dataset that consisted of 317 total training files.  These were compiled with two different compilers (gcc and clang) and a variety of compiler options.  I did not modify any default parameters for the models in scikit-learn.  As shown in Table \ref{table:kernelfunction}, the linear kernel has the best overall performance.  

For the remainder of the results, I will only use results from the SVM with a linear kernel.

\begin{center}
\begin{table}[H]
\begin{tabular}{c|cccc}
\textbf{Kernel} & \textbf{Model 1} & \textbf{Model 2} & \textbf{Model 3} & \textbf{Average}\\
\hline
Linear & .98 &.94 & .92 & .95\\
RBF & .96 & .90 & .76 & .87\\
Polynomial & .98 & .94 & .84 & .92\\
Sigmoid & .96 & .59 & .29 & .62
\end{tabular}
\caption{The accuracy of each SVM kernel function on each model}
\label{table:kernelfunction}
\end{table}
\end{center}

\subsection{Feature type results}
I evaluated each model on 4 feature types (category vs. instruction and count vs. proportion) to determine the optimal feature set, as seen in Table \ref{table:feature}.  The counting features performed nearly twice as well on the clustering task as the proportional features did, while proportional features performed better on the classification task.  There was no significant difference in the overall performance between instruction and category features.  This is not unexpected, as the category feature would encapsulate the instruction feature.

For the remainder of the results, I will only use results from the proportional and categorical feature set for the classification task, and the counting and instruction feature set for the clustering task.

\begin{center}
\begin{table}
\begin{tabular}{c|cccc}
& \textbf{Count/ins} & \textbf{Count/cat} & \textbf{Prop/ins} & \textbf{Prop/cat}\\
\hline
\textbf{Avg. cross-validation} & .939 & .927 & .987 & .984\\
\textbf{Avg. f1-score} & .860 & .837 & .961 & .972\\
\textbf{Avg. v-score} & .460 & .460 & .253 & .255\\

\end{tabular}
\caption{Summary of feature set performance as measured by average cross-validation score, f1-score, and v-score}
\label{table:feature}
\end{table}
\end{center}

\subsection{Model 1: Cryptographic detection binary classifier}
The results for detecting cryptographic algorithms with each different machine learning algorithm are shown in Table \ref{table:model1}.  It is possible that the testing data was too similar to the training data, and not general enough, resulting in biased results.

\begin{center}
\begin{table}
\begin{tabular}{c|ccc}
\textbf{Algorithm} & \textbf{Precision/Homogeneity} & \textbf{Recall/Completeness} &\textbf{F1-score/V-score}\\
\hline
SVM & 1 & 1 &1\\
Naive Bayes & 1 & .97 &.99\\
Decision Tree & 1 & 1 & 1\\
K-means & 1 & 1 & 1\\
\end{tabular}
\caption{Summary of classifier/clustering results for a model that classifies a binary program as containing crypto or not (Model 1)}\label{table:model1}
\end{table}
\end{center}

\subsection{Model 2: Cryptographic algorithm type binary classifier}
The results for classifying algorithms as encryption or hashing with each different machine learning algorithm are shown in Table \ref{table:model2}.

\begin{center}
\begin{table}
\begin{tabular}{c|ccc}
\textbf{Algorithm} & \textbf{Precision/Homogeneity} & \textbf{Recall/Completeness} &\textbf{F1-score/V-score}\\
\hline
SVM & 1 & 1& 1\\
Naive Bayes & .92 & .9& .9\\
Decision Tree & 1 & 1& 1\\
K-means & 0 & 1 & 0\\
\end{tabular}
\caption{Summary of classifier/clustering results for a model that classifies a binary program that contains crypto as having encryption or hashing (Model 2)}\label{table:model2}
\end{table}
\end{center}

\subsection{Model 3: Cryptographc algorithm multiclass classifier}
The results for classifying each cryptographic algorithm with each different machine learning algorithm are shown in Table \ref{table:model3}.
\begin{center}
\begin{table}
\begin{tabular}{c|ccc}
\textbf{Algorithm} & \textbf{Precision/Homogeneity} & \textbf{Recall/Completeness} &\textbf{F1-score/V-score}\\
\hline
SVM & 1 & 1& 1\\
Naive Bayes & .83 & .9& .86\\
Decision Tree & 1 & 1& 1\\
K-means & .26 & .71& .38 \\
\end{tabular}
\caption{Summary of classifier/clustering results for a model that classifies a binary program that contains crypto as belonging to one of 5 pre-specified algorithms (Model 3)}\label{table:model3}
\end{table}
\end{center}

\subsection{Discussion}
My experiments have demonstrated that the best SVM kernel function for this problem is a linear kernel, and that while proportional and categorical features work best for the classification task, it is better to use counting and instructional features for the clustering task.  As shown in Table \ref{table:allcv}, the decision tree models perform the best in cross-validation testing.

It is likely that the testing data is too similar to the training data, implying that the models work perfectly.  In this case, we can rely on the results of cross-validation over the entire training set (using both gcc and clang for compilation) to conclude that the models will likely generalize given better test data.  Additionally, Model 1 would benefit from training on more non-cryptographic examples.

When I evaluated the models on larger real-world programs, they performed very poorly, likely due to the difference in scale of the training and testing examples.  While my models are trained on very small programs that perform a specific task, the real examples will be large programs that perform many tasks.  In order to succeed on these real-world examples, further work is needed.

\begin{center}
\begin{table}
\begin{tabular}{c|ccc}
&\textbf{Model 1} & \textbf{Model 2} &\textbf{Model 3}\\
\hline
SVM & .984 & .944 & .927\\
Naive Bayes & .741 & .897 & .779\\
Decision Tree & .984 & .993 & .987
\end{tabular}
\caption{Cross validation results for each model and classification algorithm}
\label{table:allcv}
\end{table}
\end{center}

