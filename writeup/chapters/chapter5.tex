\chapter{Experimental Evaluation}
All experiments were run on an Ubuntu virtual machine with 1 processor core and 1024 MB of memory.


\section{Timing Analysis}
\subsection{Feature Extraction}
Due to the small size of the training set as well as the sparse nature of the feature vectors, model training is fast (less than 1 second per model).  Feature extraction requires significantly more time.  The average time to process features is slightly over 3 seconds.  I evaluated the feature extraction processing on 169 training files.

\begin{center}
\begin{table}
\begin{tabular}{c|cc}
\textbf{Feature type} & \textbf{Average processing time (seconds)} & \textbf{Total failures}\\
\hline
Instruction & 1.779 & 0\\
Category & 1.444 & 0\\
Loop & 1.654 & 59
\end{tabular}
\label{featureprocessing}
\caption{Summary of processing failures and timing analysis for feature extraction}
\end{table}
\end{center}

\section{Measures}
\subsection{Supervised learning}
The supervised algorithms were evaluated in two ways: cross-validation and test data.

\textbf{Cross-validation}: I used $k$-fold cross validation to evaluate each model.  The training data is split into $k$ equal size partitions, with $k-1$ subsets used to train the model, and the remaining subset used for validation.  This is repeated $k$ times and produces $k$ different accuracy scores, which may be averaged.

I chose $k=3$ due to my small dataset.

\textbf{See what happens when you use StratifiedKFold}

\textbf{Test data}:  To maintain the integrity of my validation analysis, I ensured that my testing and training data sets were distinct.

To test Model 1, I aggregated open source implementations of various crypto algorithms, as well as programs that do not contain crypto (\textbf{this is kinda a todo)}.

To test Models 2 and 3, I compiled my training set with gcc, then compiled the same set of files with clang.  This emulates the idea of a basic block and isolates the crypto.  When these models were tested against larger programs, they performed poorly without this isolation.

After fitting the model, I evaluated three measures of correctness: precision, recall, and f1-score (averaged over all classes).  Consider the true positives ($tp$), true negatives ($tn$), false positives ($fp$), and false negatives ($fn$).
\begin{description}
\item[Precision]: $\frac{tp}{(tp+fp)}$
\item[Recall]: $\frac{tp}{(tp+fn)}$
\item[F1-score]: The harmonic mean of the precision and real $$f1=2\times(\text{precision}\times\text{recall})/ (\text{precision}+\text{recall})$$
\end{description}

\subsection{Unsupervised learning}
To evaluate the models generated by kmeans clustering, I considered three metrics: homogeneity, completeness, and v-score.

\begin{description}
	\item[Homogeneity]: A cluster is homogeneous (value=1.0) if all clusters contain data points which belong to a single class (requires true labeled data)
	\item[Completeness]: A cluster is complete (value = 1.0) if all data points of a class are elements of the same cluster
	\item[V-score]: The harmonic mean between homogeneity and completeness $$v=2\times(\text{homogeneity}\times\text{completeness})/ (\text{homogeneity}+\text{completeness})$$
\end{description}

\section{Experimental Results}
\subsection{SVM kernel Results}
To determine the optimal kernel function, I used 3-fold cross-validation on a dataset that consisted of 317 total training files.  These were compiled with two different compilers (gcc and clang) and a variety of compiler options.  I did not modify any default parameters for the models in scikit-learn.

\begin{center}
\begin{table}
\begin{tabular}{c|cccc}
\textbf{Kernel} & \textbf{Model 1} & \textbf{Model 2} & \textbf{Model 3} & \textbf{Average}\\
\hline
Linear & .98 &.94 & .92 & .95\\
RBF & .96 & .90 & .76 & .87\\
Polynomial & .98 & .94 & .84 & .92\\
Sigmoid & .96 & .59 & .29 & .62
\end{tabular}
\caption{The accuracy of each SVM kernel function on each model}
\end{table}
\end{center}


\subsection{Model 1: Cryptographic detection binary classifier}
\begin{center}
\begin{table}
\begin{tabular}{c|ccc}
\textbf{Algorithm} & \textbf{Precision/Homogeneity} & \textbf{Recall/Completeness} &\textbf{F1-score/V-score}\\
\hline
SVM & 0 & 0\\
Naive Bayes & 0 & 0\\
Decision Tree & 0 & 0\\
K-means & 0 & 0 \\
\end{tabular}
\caption{Summary of classifier/clustering results for a model that classifies a binary program as containing crypto or not}\label{model1}
\end{table}
\end{center}

\subsection{Model 2: Cryptographic algorithm type binary classifier}
\begin{center}
\begin{table}
\begin{tabular}{c|ccc}
\textbf{Algorithm} & \textbf{Precision/Homogeneity} & \textbf{Recall/Completeness} &\textbf{F1-score/V-score}\\
\hline
SVM & 0 & 0\\
Naive Bayes & 0 & 0\\
Decision Tree & 0 & 0\\
K-means & 0 & 0 \\
\end{tabular}
\caption{Summary of classifier/clustering results for a model that classifies a binary program that contains crypto as having encryption or hashing}\label{model2}
\end{table}
\end{center}

\subsection{Model 3: Cryptographc algorithm multiclass classifier}
\begin{center}
\begin{table}
\begin{tabular}{c|ccc}
\textbf{Algorithm} & \textbf{Precision/Homogeneity} & \textbf{Recall/Completeness} &\textbf{F1-score/V-score}\\
\hline
SVM & 0 & 0\\
Naive Bayes & 0 & 0\\
Decision Tree & 0 & 0\\
K-means & 0 & 0 \\
\end{tabular}
\caption{Summary of classifier/clustering results for a model that classifies a binary program that contains crypto as belonging to one of 5 pre-specified algorithms}\label{model3}
\end{table}
\end{center}

